{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f3800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def dark_channel(im, size=15):\n",
    "    min_rgb = np.min(im, axis=2)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (size,size))\n",
    "    dark = cv2.erode(min_rgb, kernel)\n",
    "    return dark\n",
    "\n",
    "def atmospheric_light(im, dark, top_percent=0.001):\n",
    "    h, w = dark.shape\n",
    "    num_pixels = int(h*w*top_percent)\n",
    "    indices = np.argsort(dark.ravel())[-num_pixels:]\n",
    "    A = np.mean(im.reshape(-1,3)[indices], axis=0)\n",
    "    return A\n",
    "\n",
    "def transmission_estimate(im, A, size=15, omega=0.95):\n",
    "    normed = im / A\n",
    "    trans = 1 - omega * dark_channel(normed, size)\n",
    "    return trans\n",
    "\n",
    "def guided_filter(I, p, r=60, eps=1e-3):\n",
    "    src = p.astype(np.float32)\n",
    "    return cv2.ximgproc.guidedFilter(guide=I, src=src, radius=r, eps=eps)\n",
    "\n",
    "def recover(im, t, A, t0=0.1):\n",
    "    t = np.nan_to_num(t, nan=0.1, posinf=0.1, neginf=0.1)\n",
    "    t = np.clip(t, t0, 1.0)\n",
    "    J = np.empty_like(im)\n",
    "\n",
    "    for i in range(3):\n",
    "        J[:, :, i] = (im[:, :, i] - A[i]) / t + A[i]\n",
    "\n",
    "    return np.clip(J, 0, 1)\n",
    "\n",
    "def dehaze_dcp(img_path, size=15, omega=0.95, Amax = 0.95, Amin = 0.7,  gamma = 1.0, top_percent=0.001):\n",
    "    im = cv2.imread(img_path).astype(np.float64) / 255.0\n",
    "    dark = dark_channel(im, size=size)\n",
    "    A = atmospheric_light(im, dark, top_percent=top_percent)\n",
    "    A = np.clip(A, Amin, Amax)\n",
    "    t = transmission_estimate(im, A, size=size, omega=omega)\n",
    "    gray = cv2.cvtColor((im * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "    t_refined = guided_filter(gray, t)\n",
    "    J = recover(im, t_refined, A)\n",
    "    J = np.power(J, gamma)\n",
    "    return (J*255).astype(np.uint8), t_refined, dark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31f0677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509/509 [02:01<00:00,  4.20it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "input_dir = \"US-Road-Signs-71/easy/perlin_synth/images\"\n",
    "output_dir = \"US-Road-Signs-71/easy/perlin_removed/images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "size = 15\n",
    "omega = 0.95\n",
    "Amax = 0.95\n",
    "Amin=0.7\n",
    "gamma = 0.85\n",
    "top_percent=0.001\n",
    "for filename in tqdm(os.listdir(input_dir)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        img_path = os.path.join(input_dir, filename)\n",
    "        J, t_refined, dark = dehaze_dcp(img_path, size=size, omega=omega, Amax=Amax, Amin=Amin, gamma=gamma, top_percent=top_percent)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(output_path, J)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af94e295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509/509 [02:00<00:00,  4.23it/s]\n",
      "100%|██████████| 509/509 [01:58<00:00,  4.28it/s]\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"US-Road-Signs-71/easy/DL_synth_2.0/images\"\n",
    "output_dir = \"US-Road-Signs-71/easy/DL_synth_2.0_removed/images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for filename in tqdm(os.listdir(input_dir)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        img_path = os.path.join(input_dir, filename)\n",
    "        J, t_refined, dark = dehaze_dcp(img_path, size=size, omega=omega, Amax=Amax, Amin=Amin, gamma=gamma, top_percent=top_percent)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(output_path, J)\n",
    "\n",
    "input_dir = \"US-Road-Signs-71/easy/DL_synth_2.75/images\"\n",
    "output_dir = \"US-Road-Signs-71/easy/DL_synth_2.75_removed/images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for filename in tqdm(os.listdir(input_dir)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        img_path = os.path.join(input_dir, filename)\n",
    "        J, t_refined, dark = dehaze_dcp(img_path, size=size, omega=omega, Amax=Amax, Amin=Amin, gamma=gamma, top_percent=top_percent)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(output_path, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a92b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def estimate_airlight(img, num_samples=2000):\n",
    "    h, w, _ = img.shape\n",
    "    img_reshaped = img.reshape(-1, 3)\n",
    "\n",
    "    # Pick brightest pixels as airlight candidates\n",
    "    intensities = np.sum(img_reshaped, axis=1)\n",
    "    indices = np.argsort(intensities)[-num_samples:]\n",
    "    A = np.mean(img_reshaped[indices], axis=0)\n",
    "    return A\n",
    "\n",
    "def get_haze_lines(img, n_clusters=40):\n",
    "    \"\"\"Cluster colors into haze-lines (using KMeans).\"\"\"\n",
    "    h, w, _ = img.shape\n",
    "    pixels = img.reshape(-1, 3).astype(np.float32)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=3, random_state=42)\n",
    "    labels = kmeans.fit_predict(pixels)\n",
    "\n",
    "    clusters = []\n",
    "    for i in range(n_clusters):\n",
    "        idx = np.where(labels == i)[0]\n",
    "        cluster_pixels = pixels[idx]\n",
    "\n",
    "        if len(cluster_pixels) == 0:\n",
    "            continue\n",
    "\n",
    "        clusters.append({\n",
    "            \"indices\": idx,\n",
    "            \"pixels\": cluster_pixels,\n",
    "            \"color\": np.mean(cluster_pixels, axis=0)\n",
    "        })\n",
    "\n",
    "    return clusters, labels, kmeans.cluster_centers_\n",
    "\n",
    "def estimate_transmission_nonlocal(clusters, A, img_size, beta=1.0):\n",
    "    \"\"\"Estimate transmission from haze-lines.\"\"\"\n",
    "    h, w = img_size\n",
    "    t = np.zeros(h*w)\n",
    "\n",
    "    A_norm = np.linalg.norm(A)\n",
    "\n",
    "    for cl in clusters:\n",
    "        c = cl[\"color\"]\n",
    "        dist = np.linalg.norm(c - A)\n",
    "        t_val = np.exp(-beta * dist / A_norm)\n",
    "        t[cl[\"indices\"]] = t_val\n",
    "\n",
    "    return t.reshape(h, w)\n",
    "\n",
    "def guided_filter(I, p, r=60, eps=1e-3):\n",
    "    \"\"\"Use simple OpenCV guided filter.\"\"\"\n",
    "    I = I.astype(np.float32)\n",
    "    p = p.astype(np.float32)\n",
    "    return cv2.ximgproc.guidedFilter(guide=I, src=p, radius=r, eps=eps)\n",
    "\n",
    "def recover_scene(img, t, A, t0=0.1):\n",
    "    t = np.clip(t, t0, 1)\n",
    "    J = np.empty_like(img)\n",
    "\n",
    "    for i in range(3):\n",
    "        J[:, :, i] = (img[:, :, i] - A[i]) / (t+1e-9) + A[i]\n",
    "\n",
    "    return np.clip(J, 0, 255).astype(np.uint8)\n",
    "\n",
    "def non_local_dehaze(img_path, n_clusters=40, beta=1.2, refine=True):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    # 1. Estimate Atmospheric Light A\n",
    "    A = estimate_airlight(img)\n",
    "\n",
    "    # 2. Build haze-lines via KMeans\n",
    "    clusters, labels, centers = get_haze_lines(img, n_clusters=n_clusters)\n",
    "\n",
    "    # 3. Transmission per haze-line\n",
    "    t = estimate_transmission_nonlocal(clusters, A, (h, w), beta=beta)\n",
    "\n",
    "    # 4. Optional guided filter refinement\n",
    "    if refine:\n",
    "        gray = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "        t = guided_filter(gray, t.astype(np.float32), r=40, eps=1e-3)\n",
    "\n",
    "    # 5. Recover scene radiance\n",
    "    J = recover_scene(img.astype(np.float32), t, A)\n",
    "\n",
    "    return J, t, A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6310167",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"US-Road-Signs-71/easy/perlin_synth/images\"\n",
    "output_dir = \"US-Road-Signs-71/easy/perlin_removed_NON/images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for filename in tqdm(os.listdir(input_dir)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        img_path = os.path.join(input_dir, filename)\n",
    "        J, t, A = non_local_dehaze(img_path, n_clusters=60, beta=1.2, refine=True)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(output_path, cv2.cvtColor(J, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "\n",
    "input_dir = \"US-Road-Signs-71/easy/DL_synth_2.0/images\"\n",
    "output_dir = \"US-Road-Signs-71/easy/DL_synth_2.0_removed_NON/images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for filename in tqdm(os.listdir(input_dir)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        img_path = os.path.join(input_dir, filename)\n",
    "        J, t, A = non_local_dehaze(img_path, n_clusters=60, beta=1.2, refine=True)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(output_path, cv2.cvtColor(J, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "input_dir = \"US-Road-Signs-71/easy/DL_synth_2.75/images\"\n",
    "output_dir = \"US-Road-Signs-71/easy/DL_synth_2.75_removed_NON/images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for filename in tqdm(os.listdir(input_dir)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        img_path = os.path.join(input_dir, filename)\n",
    "        J, t, A = non_local_dehaze(img_path, n_clusters=60, beta=1.2, refine=True)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(output_path, cv2.cvtColor(J, cv2.COLOR_RGB2BGR))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dfb41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hybrid_dehaze_gpu.py\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "def imread_rgb_f32(path):\n",
    "    bgr = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    if bgr is None:\n",
    "        raise FileNotFoundError(path)\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    return (rgb.astype(np.float32) / 255.0)\n",
    "\n",
    "def imsave_rgb_u8(path, rgb_f32):\n",
    "    rgb = np.clip(rgb_f32, 0.0, 1.0)\n",
    "    bgr = cv2.cvtColor((rgb * 255.0).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(path, bgr)\n",
    "\n",
    "# -------------------------\n",
    "# Fast guided filter (CPU, OpenCV-based box filters)\n",
    "# -------------------------\n",
    "def fast_guided_filter(I, p, r=30, eps=1e-3):\n",
    "    \"\"\"\n",
    "    I: guidance image in [0,1] float32 (HxW)\n",
    "    p: input image to filter in [0,1] float32 (HxW)\n",
    "    returns filtered p\n",
    "    \"\"\"\n",
    "    # ensure float32\n",
    "    I = I.astype(np.float32)\n",
    "    p = p.astype(np.float32)\n",
    "    win = (r, r)\n",
    "\n",
    "    mean_I = cv2.boxFilter(I, ddepth=-1, ksize=(r,r), normalize=True)\n",
    "    mean_p = cv2.boxFilter(p, ddepth=-1, ksize=(r,r), normalize=True)\n",
    "    corr_I = cv2.boxFilter(I * I, ddepth=-1, ksize=(r,r), normalize=True)\n",
    "    corr_Ip = cv2.boxFilter(I * p, ddepth=-1, ksize=(r,r), normalize=True)\n",
    "\n",
    "    var_I = corr_I - mean_I * mean_I\n",
    "    cov_Ip = corr_Ip - mean_I * mean_p\n",
    "\n",
    "    a = cov_Ip / (var_I + eps)\n",
    "    b = mean_p - a * mean_I\n",
    "\n",
    "    mean_a = cv2.boxFilter(a, ddepth=-1, ksize=(r,r), normalize=True)\n",
    "    mean_b = cv2.boxFilter(b, ddepth=-1, ksize=(r,r), normalize=True)\n",
    "\n",
    "    q = mean_a * I + mean_b\n",
    "    return q\n",
    "\n",
    "# -------------------------\n",
    "# DCP helpers (cpu numpy)\n",
    "# -------------------------\n",
    "def dark_channel_np(im, size=15):\n",
    "    # im: HxWx3 float32 [0,1]\n",
    "    min_ch = np.min(im, axis=2)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (size, size))\n",
    "    dark = cv2.erode((min_ch * 255).astype(np.uint8), kernel)\n",
    "    return dark.astype(np.float32) / 255.0\n",
    "\n",
    "def estimate_atmospheric_light_np(im, dark, top_percent=0.001):\n",
    "    h, w = dark.shape\n",
    "    num = max(1, int(h * w * top_percent))\n",
    "    idx = np.argsort(dark.ravel())[-num:]\n",
    "    A = np.mean(im.reshape(-1, 3)[idx], axis=0)\n",
    "    return A\n",
    "\n",
    "def transmission_from_dcp_np(im, A, size=15, omega=0.95):\n",
    "    norm = im / (A[None, None, :] + 1e-8)\n",
    "    t = 1.0 - omega * dark_channel_np(norm, size)\n",
    "    return np.clip(t, 0.0, 1.0)\n",
    "\n",
    "# -------------------------\n",
    "# GPU KMeans (PyTorch) - runs on device if available\n",
    "# -------------------------\n",
    "def torch_kmeans(points, n_clusters=32, n_iters=20, device=None, init_centers=None):\n",
    "    \"\"\"\n",
    "    Simple kmeans implemented in PyTorch.\n",
    "    points: (N, 3) float32 tensor\n",
    "    returns: centers (k,3) tensor, labels (N,) long tensor\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = points.device\n",
    "\n",
    "    N, D = points.shape\n",
    "    if init_centers is None:\n",
    "        # random sample initial centers\n",
    "        idx = torch.randperm(N, device=device)[:n_clusters]\n",
    "        centers = points[idx].clone()\n",
    "    else:\n",
    "        centers = init_centers.to(device).clone()\n",
    "\n",
    "    for it in range(n_iters):\n",
    "        # compute distance in a vectorized way: (N, k)\n",
    "        # use broadcasting: (N,1,3) - (1,k,3) -> (N,k,3)\n",
    "        # but compute squared distances efficiently\n",
    "        # dist = (points.unsqueeze(1) - centers.unsqueeze(0)).pow(2).sum(2)\n",
    "        # using formula: ||x||^2 - 2 x.c + ||c||^2\n",
    "        pts_norm = (points * points).sum(1, keepdim=True)  # (N,1)\n",
    "        centers_norm = (centers * centers).sum(1).unsqueeze(0)  # (1,k)\n",
    "        dots = points @ centers.t()  # (N,k)\n",
    "        dists = pts_norm - 2.0 * dots + centers_norm  # (N,k)\n",
    "\n",
    "        labels = torch.argmin(dists, dim=1)  # (N,)\n",
    "        # update centers\n",
    "        new_centers = []\n",
    "        for k in range(n_clusters):\n",
    "            mask = (labels == k)\n",
    "            if mask.sum() == 0:\n",
    "                # reinitialize empty cluster to a random point\n",
    "                new_centers.append(points[torch.randint(0, N, (1,), device=device)].squeeze(0))\n",
    "            else:\n",
    "                new_centers.append(points[mask].mean(dim=0))\n",
    "        centers = torch.stack(new_centers, dim=0)\n",
    "    return centers, labels\n",
    "\n",
    "# -------------------------\n",
    "# Hybrid dehaze pipeline\n",
    "# -------------------------\n",
    "def hybrid_dehaze_gpu(\n",
    "    img_path,\n",
    "    out_path=None,\n",
    "    # clustering params\n",
    "    n_clusters=32,\n",
    "    sample_size=30000,\n",
    "    kmeans_iters=12,\n",
    "    # image scale for clustering (smaller -> faster)\n",
    "    scale=0.6,\n",
    "    # guided filter params\n",
    "    gf_r=40,\n",
    "    gf_eps=1e-3,\n",
    "    # DCP params (for fusion)\n",
    "    dcp_patch=15,\n",
    "    dcp_omega=0.85,\n",
    "    dcp_top_percent=0.001,\n",
    "    # recovery\n",
    "    t0=0.05,\n",
    "    device_preference=None  # \"cuda\" or \"cpu\" or None (auto)\n",
    "):\n",
    "    # 1) load image\n",
    "    im = imread_rgb_f32(img_path)  # HxWx3 float32 in [0,1]\n",
    "    h, w = im.shape[:2]\n",
    "\n",
    "    # 2) optionally downscale to speed up clustering\n",
    "    if scale < 1.0:\n",
    "        small = cv2.resize(im, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_AREA)\n",
    "    else:\n",
    "        small = im.copy()\n",
    "    hs, ws = small.shape[:2]\n",
    "\n",
    "    # 3) Prepare pixel samples for clustering (GPU)\n",
    "    pixels = (small.reshape(-1, 3).copy()).astype(np.float32)  # Nx3 numpy\n",
    "    N = pixels.shape[0]\n",
    "    # sample indices\n",
    "    if sample_size < N:\n",
    "        idx = np.random.choice(N, size=sample_size, replace=False)\n",
    "        sample_pixels = pixels[idx]\n",
    "    else:\n",
    "        sample_pixels = pixels\n",
    "\n",
    "    # 4) Move samples to torch (GPU if available and requested)\n",
    "    use_cuda = False\n",
    "    if device_preference == \"cuda\":\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "    elif device_preference == \"cpu\":\n",
    "        use_cuda = False\n",
    "    else:\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    pts = torch.from_numpy(sample_pixels).to(device)\n",
    "\n",
    "    # 5) Run kmeans on sampled points (fast on GPU)\n",
    "    centers_t, _ = torch_kmeans(pts, n_clusters=n_clusters, n_iters=kmeans_iters, device=device)\n",
    "    centers = centers_t.detach().cpu().numpy()  # kx3\n",
    "\n",
    "    # 6) Assign every (small) pixel to nearest center (vectorized on CPU for memory)\n",
    "    # For speed, do assignment in PyTorch if we are on GPU (faster)\n",
    "    if use_cuda:\n",
    "        # compute distances on GPU for full pixels\n",
    "        all_pts = torch.from_numpy(pixels).to(device)  # (N,3)\n",
    "        centers_dev = centers_t.to(device)\n",
    "        # compute squared dists using norms trick\n",
    "        ap = (all_pts * all_pts).sum(1, keepdim=True)\n",
    "        ac = (centers_dev * centers_dev).sum(1).unsqueeze(0)\n",
    "        dots = all_pts @ centers_dev.t()\n",
    "        dists = ap - 2.0 * dots + ac  # (N,k)\n",
    "        labels = torch.argmin(dists, dim=1).cpu().numpy()\n",
    "    else:\n",
    "        # CPU assign (vectorized numpy)\n",
    "        # dists: N x k, careful about memory - do in chunks\n",
    "        k = centers.shape[0]\n",
    "        labels = np.empty((N,), dtype=np.int32)\n",
    "        chunk = 200000  # safe chunk size\n",
    "        for i0 in range(0, N, chunk):\n",
    "            i1 = min(N, i0 + chunk)\n",
    "            a = pixels[i0:i1, :]  # (M,3)\n",
    "            # distance to centers\n",
    "            # use (a**2).sum(1,keepdims) - 2 a @ centers.T + centers_norm\n",
    "            a_norm = (a * a).sum(axis=1, keepdims=True)  # (M,1)\n",
    "            c_norm = (centers * centers).sum(axis=1, keepdims=True).T  # (1,k)\n",
    "            dots = a @ centers.T  # (M,k)\n",
    "            dists = a_norm - 2.0 * dots + c_norm\n",
    "            labels[i0:i1] = np.argmin(dists, axis=1)\n",
    "\n",
    "    labels_img = labels.reshape(hs, ws)\n",
    "\n",
    "    # 7) Estimate atmospheric light A (use DCP-based robust selection on small image)\n",
    "    dark_small = dark_channel_np(small, size=dcp_patch)\n",
    "    A_dcp = estimate_atmospheric_light_np(small, dark_small, top_percent=dcp_top_percent)\n",
    "    # also compute a centers-based A (brightest center)\n",
    "    center_intens = centers.sum(axis=1)\n",
    "    A_centers = centers[np.argmax(center_intens)]\n",
    "    # fuse: prefer DCP estimate but clamp near centers estimate to avoid pure-white\n",
    "    A = np.clip(A_dcp, 0.0, 0.98)\n",
    "    # small safety: if A is extremely close to 1, pull it toward center\n",
    "    if np.mean(A) > 0.95:\n",
    "        A = 0.6 * A + 0.4 * A_centers\n",
    "    A = np.clip(A, 0.4, 0.98)\n",
    "\n",
    "    # 8) Non-local transmission estimate per cluster\n",
    "    # distance of cluster center to A -> larger distance -> larger transmission (less haze)\n",
    "    centers_norm = np.linalg.norm(centers, axis=1)\n",
    "    A_norm = np.linalg.norm(A) + 1e-8\n",
    "    # compute scalar t_k per cluster using a soft mapping\n",
    "    # tune alpha to control strength\n",
    "    alpha = 2.0\n",
    "    dists_cent = np.linalg.norm(centers - A[None, :], axis=1)\n",
    "    # normalize distances to [0,1]\n",
    "    dmin, dmax = dists_cent.min(), dists_cent.max()\n",
    "    if dmax - dmin < 1e-6:\n",
    "        rel = np.zeros_like(dists_cent)\n",
    "    else:\n",
    "        rel = (dists_cent - dmin) / (dmax - dmin)\n",
    "    # map rel -> transmission using an exponential or linear mapping (we use exp to emphasize near-A)\n",
    "    t_clusters = np.clip(np.exp(-alpha * (1.0 - rel)), 0.05, 0.99)  # cluster-wise t\n",
    "\n",
    "    # fill per-pixel t on small image\n",
    "    t_small = t_clusters[labels_img]\n",
    "\n",
    "    # 9) refine transmission via guided filter on CPU\n",
    "    gray_small = cv2.cvtColor((small * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY).astype(np.float32) / 255.0\n",
    "    t_refined_small = fast_guided_filter(gray_small, t_small.astype(np.float32), r=gf_r, eps=gf_eps)\n",
    "\n",
    "    # 10) upsample transmission to full resolution\n",
    "    t_full = cv2.resize(t_refined_small, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "    t_full = np.clip(t_full, t0, 0.99)\n",
    "\n",
    "    # 11) Recover radiance on CPU\n",
    "    # ensure im is float32 [0,1]\n",
    "    J = np.empty_like(im, dtype=np.float32)\n",
    "    for c in range(3):\n",
    "        J[:, :, c] = (im[:, :, c] - A[c]) / t_full + A[c]\n",
    "    J = np.clip(J, 0.0, 1.0)\n",
    "\n",
    "    # 12) Optional fusion with DCP (blend where DCP is more reliable)\n",
    "    # compute DCP transmission\n",
    "    t_dcp_small = transmission_from_dcp_np(small, A, size=dcp_patch, omega=dcp_omega)\n",
    "    t_dcp_small = cv2.normalize(t_dcp_small, None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
    "    t_dcp_full = cv2.resize(t_dcp_small, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "    # fusion weight: where DCP suggests denser haze (low t), give more weight to DCP, else non-local\n",
    "    w_dcp = np.clip((1.0 - t_dcp_full) * 2.0, 0.0, 1.0)  # heuristic\n",
    "    # combined t\n",
    "    t_combined = w_dcp * t_dcp_full + (1.0 - w_dcp) * t_full\n",
    "    t_combined = np.clip(t_combined, t0, 0.99)\n",
    "\n",
    "    # final recovery with combined transmission\n",
    "    J_comb = np.empty_like(im, dtype=np.float32)\n",
    "    for c in range(3):\n",
    "        J_comb[:, :, c] = (im[:, :, c] - A[c]) / t_combined + A[c]\n",
    "    J_comb = np.clip(J_comb, 0.0, 1.0)\n",
    "\n",
    "    # small gamma correction for visual quality\n",
    "    J_comb = np.power(J_comb, 0.95)\n",
    "\n",
    "    # save if requested\n",
    "    if out_path:\n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        imsave_rgb_u8(out_path, J_comb)\n",
    "\n",
    "    return {\n",
    "        \"J\": J_comb,\n",
    "        \"t_full\": t_full,\n",
    "        \"t_combined\": t_combined,\n",
    "        \"A\": A,\n",
    "        \"use_cuda\": use_cuda\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "131c8df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509/509 [02:07<00:00,  4.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# device_pref = None if args.device == \"auto\" else (\"cuda\" if args.device == \"cuda\" else \"cpu\")\n",
    "input_dir = \"US-Road-Signs-71/easy/perlin_synth/images\"\n",
    "output_dir = \"US-Road-Signs-71/easy/perlin_removed_NON/images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for filename in tqdm(os.listdir(input_dir)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        img_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        res = hybrid_dehaze_gpu(\n",
    "            img_path,\n",
    "            out_path=output_path,\n",
    "            n_clusters=32,\n",
    "            scale=1.0,\n",
    "            device_preference=\"cuda\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ebb0a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509/509 [02:05<00:00,  4.04it/s]\n",
      "100%|██████████| 509/509 [02:05<00:00,  4.06it/s]\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"US-Road-Signs-71/easy/DL_synth_2.0/images\"\n",
    "output_dir = \"US-Road-Signs-71/easy/DL_synth_2.0_removed_NON/images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for filename in tqdm(os.listdir(input_dir)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        img_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        res = hybrid_dehaze_gpu(\n",
    "            img_path,\n",
    "            out_path=output_path,\n",
    "            n_clusters=32,\n",
    "            scale=1.0,\n",
    "            device_preference=\"cuda\"\n",
    "            )\n",
    "\n",
    "input_dir = \"US-Road-Signs-71/easy/DL_synth_2.75/images\"\n",
    "output_dir = \"US-Road-Signs-71/easy/DL_synth_2.75_removed_NON/images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for filename in tqdm(os.listdir(input_dir)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        img_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        res = hybrid_dehaze_gpu(\n",
    "            img_path,\n",
    "            out_path=output_path,\n",
    "            n_clusters=32,\n",
    "            scale=1.0,\n",
    "            device_preference=\"cuda\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1df1d38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:08<00:00,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"US-Road-Signs-71/collection\"\n",
    "output_dir = \"US-Road-Signs-71/easy/collection/images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "size = 15\n",
    "omega = 0.95\n",
    "Amax = 0.95\n",
    "Amin=0.7\n",
    "gamma = 1.0\n",
    "top_percent=0.01\n",
    "for filename in tqdm(os.listdir(input_dir)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.JPEG')):\n",
    "        img_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, 'hybid_' + filename )\n",
    "        res = hybrid_dehaze_gpu(\n",
    "            img_path,\n",
    "            out_path=output_path,\n",
    "            n_clusters=32,\n",
    "            scale=1.0,\n",
    "            device_preference=\"cuda\"\n",
    "            )\n",
    "        \n",
    "        output_path = os.path.join(output_dir, 'dcp_' + filename )\n",
    "        J, t_refined, dark = dehaze_dcp(img_path, size=size, omega=omega, Amax=Amax, Amin=Amin, gamma=gamma, top_percent=top_percent)\n",
    "        output_path = os.path.join(output_dir, 'dcp' + filename)\n",
    "        cv2.imwrite(output_path, J)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2099b02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509/509 [00:06<00:00, 78.96it/s]\n",
      "100%|██████████| 509/509 [00:15<00:00, 33.74it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "def histogram_equalization(image):\n",
    "    \"\"\"\n",
    "    Apply histogram equalization to enhance low-light images.\n",
    "    For color images, applies to V channel in HSV color space.\n",
    "    \"\"\"\n",
    "    if len(image.shape) == 2:\n",
    "        # Grayscale image\n",
    "        equalized = cv2.equalizeHist(image)\n",
    "    else:\n",
    "        # Color image - convert to HSV and equalize V channel\n",
    "        img_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        img_hsv[:, :, 2] = cv2.equalizeHist(img_hsv[:, :, 2])\n",
    "        equalized = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    return equalized\n",
    "\n",
    "def gamma_correction(image, gamma=0.5):\n",
    "    \"\"\"\n",
    "    Apply gamma correction with specified gamma value.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (BGR format)\n",
    "        gamma: Gamma value for correction\n",
    "    \"\"\"\n",
    "    # Build lookup table for gamma correction\n",
    "    # inv_gamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** gamma) * 255 \n",
    "                      for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    \n",
    "    # Apply gamma correction using lookup table\n",
    "    corrected = cv2.LUT(image, table)\n",
    "    return corrected\n",
    "\n",
    "def analyze_image_brightness(image):\n",
    "    \"\"\"\n",
    "    Analyze image to determine optimal parameters.\n",
    "    \"\"\"\n",
    "    # Convert to grayscale for analysis\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "    \n",
    "    # Compute statistics\n",
    "    mean_intensity = np.mean(gray)\n",
    "    std_intensity = np.std(gray)\n",
    "    median_intensity = np.median(gray)\n",
    "    \n",
    "    # Compute percentiles\n",
    "    p10 = np.percentile(gray, 10)\n",
    "    p50 = np.percentile(gray, 50)\n",
    "    p90 = np.percentile(gray, 90)\n",
    "    \n",
    "    # Determine brightness level\n",
    "    is_lowlight = mean_intensity < 80\n",
    "    is_very_lowlight = mean_intensity < 50\n",
    "    \n",
    "    # Recommend gamma based on analysis\n",
    "    if is_very_lowlight:\n",
    "        recommended_gamma = 0.4\n",
    "        reason = \"Very low mean intensity detected\"\n",
    "    elif is_lowlight:\n",
    "        recommended_gamma = 0.5\n",
    "        reason = \"Low mean intensity detected\"\n",
    "    elif mean_intensity < 120:\n",
    "        recommended_gamma = 0.6\n",
    "        reason = \"Moderately low mean intensity\"\n",
    "    else:\n",
    "        recommended_gamma = 0.8\n",
    "        reason = \"Image already fairly bright\"\n",
    "    \n",
    "    # Assess contrast\n",
    "    if std_intensity < 30:\n",
    "        contrast_level = \"Low contrast\"\n",
    "    elif std_intensity < 60:\n",
    "        contrast_level = \"Moderate contrast\"\n",
    "    else:\n",
    "        contrast_level = \"High contrast\"\n",
    "    \n",
    "    return {\n",
    "        'mean_intensity': mean_intensity,\n",
    "        'std_intensity': std_intensity,\n",
    "        'median_intensity': median_intensity,\n",
    "        'p10': p10,\n",
    "        'p50': p50,\n",
    "        'p90': p90,\n",
    "        'is_lowlight': is_lowlight,\n",
    "        'recommended_gamma': recommended_gamma,\n",
    "        'recommendation_reason': reason,\n",
    "        'contrast_level': contrast_level\n",
    "    }\n",
    "\n",
    "def select_optimal_gamma(image, gamma_range=(0.3, 0.4, 0.5, 0.6, 0.7)):\n",
    "    \"\"\"\n",
    "    Test multiple gamma values and select optimal one.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for gamma in gamma_range:\n",
    "        enhanced = gamma_correction(image, gamma)\n",
    "        gray = cv2.cvtColor(enhanced, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        mean_int = np.mean(gray)\n",
    "        std_int = np.std(gray)\n",
    "        overexposed = np.sum(gray > 245) / gray.size * 100\n",
    "        underexposed = np.sum(gray < 10) / gray.size * 100\n",
    "        \n",
    "        # Quality score: target mean ~130, penalize over-exposure\n",
    "        brightness_score = 100 - abs(mean_int - 130)\n",
    "        overexposure_penalty = overexposed * 5\n",
    "        quality_score = brightness_score - overexposure_penalty\n",
    "        \n",
    "        results[gamma] = {\n",
    "            'mean_intensity': mean_int,\n",
    "            'std_intensity': std_int,\n",
    "            'overexposed_pct': overexposed,\n",
    "            'underexposed_pct': underexposed,\n",
    "            'quality_score': quality_score\n",
    "        }\n",
    "    \n",
    "    # Select gamma with highest quality score\n",
    "    optimal_gamma = max(results.keys(), key=lambda g: results[g]['quality_score'])\n",
    "    return optimal_gamma, results\n",
    "\n",
    "ipnut_dir = \"US-Road-Signs-71/easy/low_light/images\"\n",
    "output_dir = \"US-Road-Signs-71/easy/low_light_hist/images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for filename in tqdm(os.listdir(ipnut_dir)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        img_path = os.path.join(ipnut_dir, filename)\n",
    "        image = cv2.imread(img_path)\n",
    "        enhanced = histogram_equalization(image)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(output_path, enhanced)\n",
    "    \n",
    "output_dir = \"US-Road-Signs-71/easy/low_light_gamma/images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for filename in tqdm(os.listdir(ipnut_dir)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        img_path = os.path.join(ipnut_dir, filename)\n",
    "        image = cv2.imread(img_path)\n",
    "        analysis = analyze_image_brightness(image)\n",
    "        recommended_gamma = analysis['recommended_gamma']\n",
    "        enhanced = gamma_correction(image, gamma=recommended_gamma)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(output_path, enhanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4db6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "253Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
